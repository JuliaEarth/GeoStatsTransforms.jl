# ------------------------------------------------------------------
# Licensed under the MIT License. See LICENSE in the project root.
# ------------------------------------------------------------------

# auxiliary functions and variables
uniform(h; Œª) = (h ‚â§ Œª)
triangular(h; Œª) = (h ‚â§ Œª) * (Œª - h)
epanechnikov(h; Œª) = (h ‚â§ Œª) * (Œª^2 - h^2)
const KERNFUN = Dict(:uniform => uniform, :triangular => triangular, :epanechnikov => epanechnikov)

"""
    GHC(k, Œª; nmax=2000, kern=:epanechnikov, link=:ward)

Assign labels to rows of geotable using the Geostatistical
Hierarchical Clustering (GHC) algorithm.

The approximate number of clusters `k` and the range `Œª` of
the `kern`el function determine the resulting number of clusters.
The larger the range the more connected are nearby samples.

Unlike in other clustering algorithms, the argument `k` can
be a list of integer values. In this case, each value is used
to cut the underlying tree data structure into clusters.

## Examples

```julia
GHC(5, 1.0) # request approximately 5 clusters
GHC([5,10,20], 1.0) # 5, 10 and 20 nested clusters
```

## Options

* `nmax` - Maximum number of observations to use in distance matrix
* `kern` - Kernel function (`:uniform`, `:triangular` or `:epanechnikov`)
* `link` - Linkage function (`:single`, `:average`, `:complete`, `:ward` or `:ward_presquared`)

## References

* Fouedjio, F. 2016. [A hierarchical clustering method for multivariate geostatistical data]
  (https://www.sciencedirect.com/science/article/abs/pii/S2211675316300367)

### Notes

The range parameter controls the sparsity pattern of the pairwise
distances, which can greatly affect the computational performance
of the GHC algorithm. We recommend choosing a range that is small
enough to connect nearby samples. For example, clustering data over
a 100x100 Cartesian grid with unit spacing is possible with `Œª=1.0`
or `Œª=2.0` but the problem starts to become computationally unfeasible
around `Œª=10.0` due to the density of points.
"""
struct GHC{K,‚Ñí<:Len} <: TableTransform
  k::K
  Œª::‚Ñí
  nmax::Int
  kern::Symbol
  link::Symbol
  GHC(k::K, Œª::‚Ñí, nmax, kern, link) where {K,‚Ñí<:Len} = new{K,float(‚Ñí)}(k, Œª, nmax, kern, link)
end

function GHC(k, Œª::Len; nmax=2000, kern=:epanechnikov, link=:ward)
  # sanity checks
  @assert all(>(0), k) "number of clusters must be positive"
  @assert Œª > zero(Œª) "kernel range must be positive"
  @assert nmax > 0 "maximum number of observations must be positive"
  @assert kern ‚àà [:uniform, :triangular, :epanechnikov] "invalid kernel function"
  @assert link ‚àà [:single, :average, :complete, :ward, :ward_presquared] "invalid linkage function"
  GHC(k, Œª, nmax, kern, link)
end

GHC(k, Œª; kwargs...) = GHC(k, _addunit(Œª, u"m"); kwargs...)

function apply(transform::GHC, geotable::AbstractGeoTable)
  # GHC parameters
  k = transform.k
  Œª = transform.Œª
  nmax = transform.nmax
  kern = transform.kern
  link = transform.link

  # all covariates must be continuous
  cond = x -> elscitype(x) <: Continuous
  msg = "GHC only defined for continuous variables"
  values(geotable) |> Assert(; cond, msg)

  # sub-sample geotable to fit dissimilarity matrix
  gtb = ghc_subsample(geotable, nmax)

  # dissimilarity matrix
  D = ghc_dissimilarity_matrix(gtb, kern, Œª)

  # classical hierarchical clustering
  tree = hclust(D, linkage=link)

  # cut tree in clusters
  newgeotables = map(eachindex(k)) do i
    # perform tree cut
    cname = Symbol("label", i)
    cvals = cutree(tree, k=k[i])

    # georeference clusters
    newgtb = georef((; cname => cvals), domain(gtb))

    # interpolate in case of sub-sampling
    interp = if nrow(newgtb) < nrow(geotable)
      InterpolateNeighbors(domain(geotable), model=NN())
    else
      Identity()
    end
    newgtb |> interp
  end

  newgeotable = if length(newgeotables) > 1
    reduce(hcat, newgeotables)
  else
    first(newgeotables) |> Rename("label1" => "label")
  end

  newgeotable, nothing
end

function ghc_subsample(geotable, nmax)
  nobs = nrow(geotable)
  tran = nobs > nmax ? Sample(nmax, replace=false) : Identity()
  geotable |> tran
end

function ghc_dissimilarity_matrix(geotable, kern, Œª)
  # retrieve domain/table
  ùíü = domain(geotable)
  ùíØ = values(geotable)

  # kernel matrix
  K = ghc_kern_matrix(kern, Œª, ùíü)

  # standardize features
  ùíÆ = ùíØ |> ZScore()

  # retrieve feature columns
  cols = Tables.columns(ùíÆ)
  vars = Tables.columnnames(cols)

  # number of covariates
  p = length(vars)

  # number of observations
  n = size(K, 1)

  # dissimilarity matrix
  D = zeros(n, n)
  @inbounds for j in 1:p # for each pair of covariates
    z‚±º = Tables.getcolumn(cols, j)
    for i in j:p
      z·µ¢ = Tables.getcolumn(cols, i)

      # difference matrix for covariate pair
      Œî = ghc_diff_matrix(z·µ¢, z‚±º)

      # contribution to dissimilarity matrix
      for l in 1:n
        K‚Çó = K[:, l]
        for k in (l + 1):n
          K‚Çñ = K[:, k]
          K‚Çñ‚Çó = kron(K‚Çó, K‚Çñ) # faster K‚Çñ * transpose(K‚Çó)
          I, W = findnz(K‚Çñ‚Çó)
          num = sum(W .* Œî[I], init=zero(eltype(W)))
          den = sum(W, init=zero(eltype(W)))
          iszero(den) || (D[k, l] += (1 / 2) * (num / den))
        end
        D[l, l] = 0.0
        for k in 1:(l - 1)
          D[k, l] = D[l, k] # leverage symmetry
        end
      end
    end
  end

  D
end

function ghc_kern_matrix(kern, Œª, ùíü)
  # kernel function
  fn = KERNFUN[kern]
  KŒª(h) = fn(h, Œª=Œª)

  # collect coordinates
  coords = [to(centroid(ùíü, i)) for i in 1:nelements(ùíü)]

  # lag matrix
  H = pairwise(Euclidean(), coords)

  # kernel matrix
  K = ustrip.(KŒª.(H))

  # return sparse matrix
  sparse(K)
end

function ghc_diff_matrix(z·µ¢, z‚±º)
  n = length(z·µ¢)
  Œî = zeros(n, n)
  @inbounds for l in 1:n
    for k in (l + 1):n
      Œî[k, l] = (z·µ¢[k] - z·µ¢[l]) * (z‚±º[k] - z‚±º[l])
    end
    Œî[l, l] = 0.0
    for k in 1:(l - 1)
      Œî[k, l] = Œî[l, k] # leverage symmetry
    end
  end
  Œî
end
