# ------------------------------------------------------------------
# Licensed under the MIT License. See LICENSE in the project root.
# ------------------------------------------------------------------

# auxiliary functions and variables
uniform(h; Î») = (h â‰¤ Î»)
triangular(h; Î») = (h â‰¤ Î») * (Î» - h)
epanechnikov(h; Î») = (h â‰¤ Î») * (Î»^2 - h^2)
const KERNFUN = Dict(:uniform => uniform, :triangular => triangular, :epanechnikov => epanechnikov)

"""
    GHC(k, Î»; nmax=2000, kern=:epanechnikov, link=:ward, as=:cluster)

A transform for partitioning geospatial data into `k` clusters 
according to a range `Î»` using Geostatistical Hierarchical
Clustering (GHC). The larger the range the more connected
are nearby samples.

## Parameters

* `k`    - Approximate number of clusters
* `Î»`    - Approximate range of kernel function in length units
* `nmax` - Maximum number of observations to use in dissimilarity matrix
* `kern` - Kernel function (`:uniform`, `:triangular` or `:epanechnikov`)
* `link` - Linkage function (`:single`, `:average`, `:complete`, `:ward` or `:ward_presquared`)
* `as`   - Variable name used to store clustering results

## References

* Fouedjio, F. 2016. [A hierarchical clustering method for multivariate geostatistical data]
  (https://www.sciencedirect.com/science/article/abs/pii/S2211675316300367)

### Notes

- The range parameter controls the sparsity pattern of the pairwise
  distances, which can greatly affect the computational performance
  of the GHC algorithm. We recommend choosing a range that is small
  enough to connect nearby samples. For example, clustering data over
  a 100x100 Cartesian grid with unit spacing is possible with `Î»=1.0`
  or `Î»=2.0` but the problem starts to become computationally unfeasible
  around `Î»=10.0` due to the density of points.
"""
struct GHC{â„’<:Len} <: ClusteringTransform
  k::Int
  Î»::â„’
  nmax::Int
  kern::Symbol
  link::Symbol
  as::Symbol
  GHC(k, Î»::â„’, nmax, kern, link, as) where {â„’<:Len} = new{float(â„’)}(k, Î», nmax, kern, link, as)
end

function GHC(k, Î»::Len; nmax=2000, kern=:epanechnikov, link=:ward, as=:cluster)
  # sanity checks
  @assert k > 0 "number of cluster must be positive"
  @assert Î» > zero(Î») "kernel range must be positive"
  @assert nmax > 0 "maximum number of observations must be positive"
  @assert kern âˆˆ [:uniform, :triangular, :epanechnikov] "invalid kernel function"
  @assert link âˆˆ [:single, :average, :complete, :ward, :ward_presquared] "invalid linkage function"
  GHC(k, Î», nmax, kern, link, Symbol(as))
end

GHC(k, Î»; kwargs...) = GHC(k, _addunit(Î», u"m"); kwargs...)

function apply(transform::GHC, geotable)
  # GHC parameters
  k = transform.k
  Î» = transform.Î»
  nmax = transform.nmax
  kern = transform.kern
  link = transform.link

  # all covariates must be continuous
  values(geotable) |> Assert(cond=x -> elscitype(x) <: Continuous)

  # sub-sample geotable to fit dissimilarity matrix
  gtb = ghc_subsample(geotable, nmax)

  # dissimilarity matrix
  D = ghc_dissimilarity_matrix(gtb, kern, Î»)

  # classical hierarchical clustering
  tree = hclust(D, linkage=link)

  # cut tree to produce clusters
  labels = cutree(tree, k=k)

  # georeference categorical labels
  newtab = (; transform.as => categorical(labels))
  newgtb = georef(newtab, domain(gtb))

  # interpolate neighbors in case of sub-sampling
  newgeotable = if nrow(newgtb) < nrow(geotable)
    newgtb |> InterpolateNeighbors(domain(geotable), model=NN())
  else
    newgtb
  end

  newgeotable, nothing
end

function ghc_subsample(geotable, nmax)
  nobs = nrow(geotable)
  tran = nobs > nmax ? Sample(nmax, replace=false) : Identity()
  geotable |> tran
end

function ghc_dissimilarity_matrix(geotable, kern, Î»)
  # retrieve domain/table
  ğ’Ÿ = domain(geotable)
  ğ’¯ = values(geotable)

  # kernel matrix
  K = ghc_kern_matrix(kern, Î», ğ’Ÿ)

  # features must be standardized
  ğ’® = ghc_standardize(ğ’¯)

  # retrieve feature columns
  cols = Tables.columns(ğ’®)
  vars = Tables.columnnames(cols)

  # number of covariates
  p = length(vars)

  # number of observations
  n = size(K, 1)

  # dissimilarity matrix
  D = zeros(n, n)
  @inbounds for j in 1:p # for each pair of covariates
    zâ±¼ = Tables.getcolumn(cols, j)
    for i in j:p
      záµ¢ = Tables.getcolumn(cols, i)

      # difference matrix for covariate pair
      Î” = ghc_diff_matrix(záµ¢, zâ±¼)

      # contribution to dissimilarity matrix
      for l in 1:n
        Kâ‚— = K[:, l]
        for k in (l + 1):n
          Kâ‚– = K[:, k]
          Kâ‚–â‚— = kron(Kâ‚—, Kâ‚–) # faster Kâ‚– * transpose(Kâ‚—)
          I, W = findnz(Kâ‚–â‚—)
          num = sum(W .* Î”[I], init=zero(eltype(W)))
          den = sum(W, init=zero(eltype(W)))
          iszero(den) || (D[k, l] += (1 / 2) * (num / den))
        end
        D[l, l] = 0.0
        for k in 1:(l - 1)
          D[k, l] = D[l, k] # leverage symmetry
        end
      end
    end
  end

  D
end

function ghc_standardize(ğ’¯)
  cols = Tables.columns(ğ’¯)
  vars = Tables.columnnames(cols)
  zstd = map(vars) do var
    z = Tables.getcolumn(cols, var)
    Î¼ = mean(z)
    Ïƒ = std(z, mean=Î¼)
    iszero(Ïƒ) ? zero(Î¼) : (z .- Î¼) ./ Ïƒ
  end
  (; zip(vars, zstd)...) |> Tables.materializer(ğ’¯)
end

function ghc_kern_matrix(kern, Î», ğ’Ÿ)
  # kernel function
  fn = KERNFUN[kern]
  KÎ»(h) = fn(h, Î»=Î»)

  # collect coordinates
  coords = [to(centroid(ğ’Ÿ, i)) for i in 1:nelements(ğ’Ÿ)]

  # lag matrix
  H = pairwise(Euclidean(), coords)

  # kernel matrix
  K = ustrip.(KÎ».(H))

  # return sparse matrix
  sparse(K)
end

function ghc_diff_matrix(záµ¢, zâ±¼)
  n = length(záµ¢)
  Î” = zeros(n, n)
  @inbounds for l in 1:n
    for k in (l + 1):n
      Î”[k, l] = (záµ¢[k] - záµ¢[l]) * (zâ±¼[k] - zâ±¼[l])
    end
    Î”[l, l] = 0.0
    for k in 1:(l - 1)
      Î”[k, l] = Î”[l, k] # leverage symmetry
    end
  end
  Î”
end
