# ------------------------------------------------------------------
# Licensed under the MIT License. See LICENSE in the project root.
# ------------------------------------------------------------------

# auxiliary functions and variables
uniform(h; Œª) = (h ‚â§ Œª)
triangular(h; Œª) = (h ‚â§ Œª) * (Œª - h)
epanechnikov(h; Œª) = (h ‚â§ Œª) * (Œª^2 - h^2)
const KERNFUN = Dict(:uniform => uniform, :triangular => triangular, :epanechnikov => epanechnikov)

"""
    GHC(k, Œª; kern=:epanechnikov, link=:ward, as=:CLUSTER)

A transform for partitioning geospatial data into `k` clusters 
according to a range `Œª` using Geostatistical Hierarchical
Clustering (GHC). The larger the range the more connected
are nearby samples.

## Parameters

* `k`    - Approximate number of clusters
* `Œª`    - Approximate range of kernel function
* `kern` - Kernel function (`:uniform`, `:triangular` or `:epanechnikov`)
* `link` - Linkage function (`:single`, `:average`, `:complete`, `:ward` or `:ward_presquared`)
* `as`   - Cluster column name

## References

* Fouedjio, F. 2016. [A hierarchical clustering method for multivariate geostatistical data]
  (https://www.sciencedirect.com/science/article/abs/pii/S2211675316300367)

### Notes

- The range parameter controls the sparsity pattern of the pairwise
  distances, which can greatly affect the computational performance
  of the GHC algorithm. We recommend choosing a range that is small
  enough to connect nearby samples. For example, clustering data over
  a 100x100 Cartesian grid with unit spacing is possible with `Œª=1.0`
  or `Œª=2.0` but the problem starts to become computationally unfeasible
  around `Œª=10.0` due to the density of points.
"""
struct GHC <: ClusteringTransform
  k::Int
  Œª::Float64
  kern::Symbol
  link::Symbol
  as::Symbol
end

function GHC(k, Œª; kern=:epanechnikov, link=:ward, as=:CLUSTER)
  # sanity checks
  @assert k > 0 "invalid number of clusters"
  @assert Œª > 0 "invalid kernel range"
  @assert kern ‚àà [:uniform, :triangular, :epanechnikov] "invalid kernel function"
  @assert link ‚àà [:single, :average, :complete, :ward, :ward_presquared] "invalid linkage function"
  GHC(k, Œª, kern, link, Symbol(as))
end

function apply(transform::GHC, geotable)
  # GHC parameters
  k = transform.k
  Œª = transform.Œª
  kern = transform.kern
  link = transform.link

  # all covariates must be continuous
  values(geotable) |> SciTypeAssertion{Continuous}()

  # dissimilarity matrix
  D = ghc_dissimilarity_matrix(geotable, kern, Œª)

  # classical hierarchical clustering
  tree = hclust(D, linkage=link)

  # cut tree to produce clusters
  labels = cutree(tree, k=k)

  newtable = (; transform.as => categorical(labels))
  newgeotable = georef(newtable, domain(geotable))

  newgeotable, nothing
end

function ghc_dissimilarity_matrix(geotable, kern, Œª)
  # retrieve domain/table
  ùíü = domain(geotable)
  ùíØ = values(geotable)

  # kernel matrix
  K = ghc_kernel_matrix(kern, Œª, ùíü)

  # difference matrices
  Œî = ghc_diff_matrices(ùíØ)

  # sum of cross-variograms
  Œì = ghc_variogram_sum(K, Œî)
end

function ghc_kernel_matrix(kern, Œª, ùíü)
  # kernel function
  fn = KERNFUN[kern]
  KŒª(h) = fn(h, Œª=Œª)

  # collect coordinates
  coords = [coordinates(centroid(ùíü, i)) for i in 1:nelements(ùíü)]

  # lag matrix
  H = pairwise(Euclidean(), coords)

  # kernel matrix
  K = KŒª.(H)

  # return sparse version
  sparse(K)
end

function ghc_diff_matrices(ùíØ)
  # features must be standardized
  ùíÆ = ghc_standardize(ùíØ)

  # retrieve standardized features
  cols = Tables.columns(ùíÆ)
  vars = Tables.columnnames(cols)

  # distance matrices
  D = map(vars) do var
    z = Tables.getcolumn(cols, var)
    pairwise(Euclidean(), z)
  end

  # number of covariates
  p = length(vars)

  # one matrix per covariate pair
  Œî = Matrix{Matrix{Float64}}(undef, p, p)
  @inbounds for j in 1:p
    for i in (j + 1):p
      Œî[i, j] = D[i] .* D[j]
    end
    Œî[j, j] = D[j] .* D[j]
    for i in 1:(j - 1)
      Œî[i, j] = Œî[j, i] # leverage the symmetry
    end
  end

  Œî
end

function ghc_variogram_sum(K, Œî)
  n = size(K, 1)
  Œì = zeros(n, n)
  for Œî‚Çí in Œî # for each covariate pair
    # update lower triangular matrix
    @inbounds for j in 1:n
      kj = K[:, j]
      for i in (j + 1):n
        ki = K[:, i]
        Kij = kron(ki, kj)
        I, W = findnz(Kij)
        num = sum(W .* Œî‚Çí[I], init=0.0)
        den = sum(W, init=0.0)
        iszero(den) || (Œì[i, j] += (1 / 2) * (num / den))
      end
    end
  end

  # mirror upper triangular matrix
  @inbounds for j in 1:n
    Œì[j, j] = 0.0
    for i in 1:(j - 1)
      Œì[i, j] = Œì[j, i] # leverage the symmetry
    end
  end

  Œì
end

function ghc_standardize(ùíØ)
  cols = Tables.columns(ùíØ)
  vars = Tables.columnnames(cols)
  zstd = map(vars) do var
    z = Tables.getcolumn(cols, var)
    Œº = mean(z)
    œÉ = std(z, mean=Œº)
    iszero(œÉ) ? zero(Œº) : (z .- Œº) ./ œÉ
  end
  (; zip(vars, zstd)...) |> Tables.materializer(ùíØ)
end
